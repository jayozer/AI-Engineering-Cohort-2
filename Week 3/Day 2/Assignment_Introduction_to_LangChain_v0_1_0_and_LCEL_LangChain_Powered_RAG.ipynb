{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LangChain v0.1.0 and LCEL: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Initialize a Simple Chain using LCEL\n",
        "  4. Implement Naive RAG using LCEL\n",
        "  5. Add and retrieve from Pinecone\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Create a Simple RAG Application Using QDrant, OpenAI, and LCEL\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVHd6POM0JFN"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "One of the [key features](https://blog.langchain.dev/langchain-v0-1-0/) of LangChain v0.1.0 is the compartmentalization of the various LangChain ecosystem packages.\n",
        "\n",
        "Instead of one all encompassing Python package - LangChain has a `core` package and a number of additional supplementary packages.\n",
        "\n",
        "We'll start by grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCC2AR-Q0m0x",
        "outputId": "fe623cdf-cc3d-4ddd-fc8e-9405c5be8c50"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet langchain langchain-core langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ELHQjQ1PYs"
      },
      "source": [
        "Now we can get our Qdrant dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XeYI9P1OXO",
        "outputId": "6384050b-85e9-44b5-bc91-18608fe62cd0"
      },
      "outputs": [],
      "source": [
        "! pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iesey9OGCKJx"
      },
      "source": [
        "Let's finally get `tiktoken` and `pymupdf` so we can leverage them later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5qIUrFuENrS",
        "outputId": "1e6e0c65-b0e6-4997-8dc0-8e1528c27b8f"
      },
      "outputs": [],
      "source": [
        "! pip install -qU tiktoken pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "We'll be leveraging OpenAI's suite of APIs - so we'll set our `OPENAI_API_KEY` `env` variable here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pKAfycq73wE",
        "outputId": "13af9f4f-eb7b-466c-d6a0-0aefe9061cf0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_xp54wIA56_"
      },
      "source": [
        "## Task 3: Initialize a Simple Chain using LCEL\n",
        "\n",
        "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyGdhbS6SkD1"
      },
      "source": [
        "### LLM Orchestration Tool (LangChain)\n",
        "\n",
        "Let's dive right into [LangChain](https://www.langchain.com/)!\n",
        "\n",
        "The first thing we want to do is create an object that lets us access OpenAI's `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "3Uj6SorxMj8e"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsmiieEh_Ye-"
      },
      "source": [
        "####â“ Question #1:\n",
        "\n",
        "What specific model are we using when we point to `gpt-3.5-turbo`?\n",
        "\n",
        "> HINT: Check out [this page](https://platform.openai.com/docs/models/gpt-3-5-turbo) to find the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer 1:\n",
        "gpt-3.5-turbo-0125\t- Updated GPT 3.5 Turbo - 16K context window - nice! but not up to date, trnaing data up to Sept 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nU8SlHfH41T"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcMKLZWBVYm7"
      },
      "source": [
        "Now, we'll set up a prompt template - more specifically a `ChatPromptTemplate`. This will let us build a prompt we can modify when we call our LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "Z770j4zPS3o5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"You are Andy. An astronomer who studied Astro physics. You are a clever cool dude who likes house music. Answer questions in a sophisticated manner and explain the reasons why. Ask questions to the user making comparisons to Astro physics and house music that will make them think about the subject further.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),  # system message\n",
        "    (\"human\", human_template)       # User message\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGku_c2VVyd_"
      },
      "source": [
        "### Our First Chain\n",
        "\n",
        "Now we can set up our first chain!\n",
        "\n",
        "#### A chain is simply two components that feed directly into eachother in a sequential fashion!\n",
        "\n",
        "You'll notice that we're using the pipe operator `|` to connect our `chat_prompt` to our `llm`.\n",
        "\n",
        "This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.\n",
        "\n",
        "You can read more about it [here](https://python.langchain.com/docs/expression_language/), but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):\n",
        "\n",
        "- **Async, Batch, and Streaming Support** Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "- **Fallbacks** The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "- **Parallelism** Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "In the following code cell we have two components:\n",
        "\n",
        "- `chat_prompt`, which is a formattable `ChatPromptTemplate` that contains a system message and a human message.\n",
        "\n",
        "We'd like to be able to pass our own `content` (as found in our `human_template`) and then have the resulting message pair sent to our model and responded to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "RcJyqOiwVt04"
      },
      "outputs": [],
      "source": [
        "chain = chat_prompt | openai_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_kHCjlL_01"
      },
      "source": [
        "Notice the pattern here:\n",
        "\n",
        "We invoke our chain with the `dict` `{\"content\" : \"Hello world!\"}`.\n",
        "\n",
        "It enters our chain:\n",
        "\n",
        "`{\"content\" : \"Hello world!\"}` -> `invoke()` -> `chat_prompt`\n",
        "\n",
        "Our `chat_prompt` returns a `PromptValue`, which is the formatted prompt. We then \"pipe\" the output of our `chat_prompt` into our `llm`.\n",
        "\n",
        "`PromptValue` -> `|` -> `llm`\n",
        "\n",
        "Our `llm` then takes the list of messages and provides an output which is return as a `str`!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqr2QuMtIjn",
        "outputId": "8dd76a83-199a-44dc-8120-93210815908b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello! How can I assist you today in exploring the wonders of the universe and the mesmerizing world of house music?' response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 72, 'total_tokens': 96}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-69bede0b-1f43-426d-b9c7-4d452baeb82e-0'\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"content\": \"Hello world!\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2znL48ECNteM"
      },
      "source": [
        "Let's try it out with a different prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjiTNeYXUCAB",
        "outputId": "1a0da3d2-c723-4421-a0d6-c6e36d5caeb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry to hear about your toddler's accident. In this situation, it's important to prioritize your child's dental health. While I'm not a medical professional, I recommend seeking immediate dental care from a local dentist or emergency clinic to assess the severity of the chip and ensure there is no risk of infection or further damage.\\n\\nAs an astronomer who studies Astro physics, I often encounter unexpected phenomena that require immediate attention and precise analysis. In this case, just as in the study of celestial bodies, it's crucial to address the issue promptly to prevent any complications.\\n\\nOnce your child has received the necessary care, you may also want to consider reaching out to your healthcare provider in the United States to inquire about coverage for any follow-up treatment that may be needed upon your return.\\n\\nDo you think there are parallels between handling unexpected situations in everyday life and addressing unforeseen events in the realm of Astro physics or even in the rhythm of house music?\", response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 112, 'total_tokens': 301}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe2043e8-82f6-4092-a7a4-4a1d685731b5-0')"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"content\" : \"My family and I are oon vacationa and my twp year old chipped hid teeth. We dont have insurance in this country and will be back to United States this Saturday. What can we do meanwhile?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THcMz8YAWsjP"
      },
      "source": [
        "Notice how we specifically referenced our `content` format option!\n",
        "\n",
        "Now that we have the basics set up - let's see what we mean by \"Retrieval Augmented\" Generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7o8aXbhRAPe"
      },
      "source": [
        "## Naive RAG - Manually Adding Context\n",
        "\n",
        "Let's look at how our model performs at a simple task - defining what LangChain is!\n",
        "\n",
        "We'll redo some of our previous work to change the `system_template` to be less...verbose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu_Uox_pPKaf",
        "outputId": "d0c54832-fead-4427-fb19-cd7969d8e882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain is a multilingual blockchain platform that aims to bridge language barriers in the blockchain and cryptocurrency space. It enables users to access information and participate in transactions in various languages, making the technology more inclusive and accessible to a global audience.' response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 22, 'total_tokens': 69}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-df9f8bb9-e2f2-417a-bc63-302ab3597cf6-0'\n"
          ]
        }
      ],
      "source": [
        "system_template = \"You are a helpful assistant.\"\n",
        "human_template = \"{content}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"content\" : \"Please define LangChain.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KXqGI4XbMb"
      },
      "source": [
        "Well, that's not very good - is it!\n",
        "\n",
        "The issue at play here is that our model was not trained on the idea of \"LangChain\", and so it's left with nothing but a guess - definitely not what we want the answer to be!\n",
        "\n",
        "Let's ask another simple LangChain question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRG5LwYoXnsr",
        "outputId": "94d61815-6a91-4c32-fc70-8b16c2366dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LECL) is a domain-specific language developed by LangChain for expressing smart contracts and blockchain operations. It is designed to be user-friendly and expressive, making it easier for developers to write code for blockchain applications. LECL provides a set of predefined functions and syntax that simplify the process of defining and executing smart contracts on the blockchain.' response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 27, 'total_tokens': 98}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-91cab79e-5eec-4c1a-9618-8bbccc71003e-0'\n"
          ]
        }
      ],
      "source": [
        "print(chat_chain.invoke({\"content\" : \"What is LangChain Expression Language (LECL)?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63pr0fgYXxC3"
      },
      "source": [
        "While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!\n",
        "\n",
        "However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgr25HjgYHwh",
        "outputId": "c62107bf-9d01-43bc-8ec0-6499e422aec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It offers benefits such as async, batch, and streaming support, fallback handling for errors, parallelism for running components in parallel, and seamless integration with LangSmith Tracing for better observability and debuggability.' response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 274, 'total_tokens': 336}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-6f8d52ea-df33-46a2-8c36-078222902bb2-0'\n"
          ]
        }
      ],
      "source": [
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "CONTEXT = \"\"\"\n",
        "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
        "\n",
        "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | openai_chat_model\n",
        "\n",
        "print(chat_chain.invoke({\"query\" : \"What is LangChain Expression Language?\", \"context\" : CONTEXT}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppQdtCedY7C4"
      },
      "source": [
        "You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!\n",
        "\n",
        "> NOTE: While RAG is an effective strategy to *help* ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes\n",
        "\n",
        "That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!\n",
        "\n",
        "However, manually providing context is not scalable - and doesn't really offer any benefit.\n",
        "\n",
        "Enter: Retrieval Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFmdARsVBJUq"
      },
      "source": [
        "## Task #2: Implement Naive RAG using LCEL\n",
        "\n",
        "Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4AozVoEZveK"
      },
      "source": [
        "## Putting the R in RAG: Retrieval 101\n",
        "\n",
        "In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.\n",
        "\n",
        "Let's tackle an immediate problem first: The Context Window.\n",
        "\n",
        "All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.\n",
        "\n",
        "Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "PbXBxffibeyp"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "EVERY HITCHHIKER'S GUIDE BOOK\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvgFuaXcHFT"
      },
      "source": [
        "We can leverage our tokenizer to count the number of tokens for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "HaKPOdSjbifn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDiSMxpE4Xi",
        "outputId": "886fd517-9128-45ca-9fbd-5152ae7f146b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(enc.encode(CONTEXT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUuZpAicLdm"
      },
      "source": [
        "The full set comes in at a whopping *636,144* tokens.\n",
        "\n",
        "So, we have too much context. What can we do?\n",
        "\n",
        "Well, the first thing that might enter your mind is: \"Use a model with more context window\", and we could definitely do that! However, even `gpt-4-32k` wouldn't be able to fit that whole text in the context window at once.\n",
        "\n",
        "So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.\n",
        "\n",
        "We have another problem now.\n",
        "\n",
        "If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!\n",
        "\n",
        "> NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no \"one size fits all\" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!\n",
        "\n",
        "In order to conceptualize the following processes - let's create a toy context set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPCiOPwUfbqn"
      },
      "source": [
        "### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 100 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "nLW9AfDKfVHn"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "nPYPBe2ngT9N"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_RGVlTihaQX",
        "outputId": "2791edb4-89d1-4772-afe4-77e7618e669d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTYny2xchS_Z",
        "outputId": "5b5c1858-9302-48a6-f2ea-441e3cd6c4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "----\n",
            "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "----\n",
            "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(chunks):\n",
        "  if i >= 5:\n",
        "    break\n",
        "  print(chunk)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Split with double new lines which works good since we keep the information together for each chunk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hOgu5Yhefv"
      },
      "source": [
        "As is shown in our result, we've split each section into 100 token chunks - cleanly separated by `\\n\\n` characters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTiJ2utMpqq"
      },
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "1. Paragraph-based splitting:\n",
        "Split the document into individual paragraphs. Each paragraph becomes a separate chunk. This may work good for shorter blogs.\n",
        "2. Semantic splitting:\n",
        "Use natural language processing techniques, such as named entity recognition or topic modeling, to identify semantically coherent chunks.\n",
        "Split the document based on semantic boundaries, such as changes in topic, entity, or context. this can work better for longer text.\n",
        "3. Do preprocessing on the document. For example add markdown to seperate titles and paragraphs. This could work well for Q/A type fo retrieval. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj18Rjafhp7d"
      },
      "source": [
        "## Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
        "\n",
        "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
        "\n",
        "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMtraxPjglX"
      },
      "source": [
        "### Why Do We Even Need Embeddings?\n",
        "\n",
        "In order to fully understand what Embeddings are, we first need to understand why we have them!\n",
        "\n",
        "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
        "\n",
        "They need numeric inputs.\n",
        "\n",
        "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
        "\n",
        "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
        "\n",
        "So, we need to come up with a process that does these two things well:\n",
        "\n",
        "- Convert non-numeric data into numeric-data\n",
        "- Capture potential semantic relationships between individual pieces of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSyKc5vkjjNE"
      },
      "source": [
        "### How Do Embeddings Capture Semantic Relationships?\n",
        "\n",
        "In a simplified sense, embeddings map a word or phrase into n-dimensional space with a dense continuous vector, where each dimension in the vector represents some \"latent feature\" of the data.\n",
        "\n",
        "This is best represented in a classic example:\n",
        "\n",
        "![image](https://i.imgur.com/K5eQtmH.png)\n",
        "\n",
        "As can be seen in the extremely simplified example: The X_1 axis represents age, and the X_2 axis represents hair.\n",
        "\n",
        "The relationship of \"puppy -> dog\" reflects the same relationship as \"baby -> adult\", but dogs are (typically) hairier than humans. However, adults typically have more hair than babies - so they are shifted slightly closer to dogs on the X_2 axis!\n",
        "\n",
        "Now, this is a simplified and contrived example - but it is *essentially* the mechanism by which embeddings capture semantic information.\n",
        "\n",
        "In reality, the dimensions don't sincerely represent hard-concepts like \"age\" or \"hair\", but it's useful as a way to think about how the semantic relationships are captured.\n",
        "\n",
        "Alright, with some history behind us - let's examine how these might help us choose relevant context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKOOY91Onern"
      },
      "source": [
        "Let's begin with a simple example - simply looking at how close to embedding vectors are for a given phrase.\n",
        "\n",
        "When we use the term \"close\" in this notebook - we're referring to a distance measure called \"cosine similarity\".\n",
        "\n",
        "We discussed above that if two embeddings are close - they are semantically similar, cosine similarity gives us a quick way to measure how similar two vectors are!\n",
        "\n",
        "Closeness is measured from 1 to -1, with 1 being extremely close and -1 being extremely close to opposite in meaning.\n",
        "\n",
        "Let's implement it with Numpy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "EVnhmxF4iD7P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okr5PMwBpjYE"
      },
      "source": [
        "We're going to be using OpenAI's `text-embedding-3-small` today.\n",
        "\n",
        "In order to choose our embeddings model, we'll refer to the MTEB leaberboard - which can be found [here](https://huggingface.co/spaces/mteb/leaderboard)!\n",
        "\n",
        "The basic logic is: We sort by our desired task - in this case `Retrieval Average (15 Datasets)`, and we're going to pick a model that performs well on that task - to keep cost in mind, we'll go with the `text-embedding-3-small` over the `text-embedding-3-large` since there's only a separation of ~5 points between the two on this task - but the cost is a significant factor less for the `small` version of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "quNjOLWspOVN"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3Qpbs2pxj_"
      },
      "source": [
        "Let's grab some vectors and see how they're related!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "C_uPM1llpSVt"
      },
      "outputs": [],
      "source": [
        "puppy_vec = embedding_model.embed_query(\"puppy\")\n",
        "dog_vec = embedding_model.embed_query(\"dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OMzg_Pyp4qV"
      },
      "source": [
        "Let's do a quick check to ensure they're all the correct dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGZ92hm9IeX"
      },
      "source": [
        "####â“ Question #2:\n",
        "\n",
        "What is the embedding dimension, given that we're using `text-embedding-3-small`?\n",
        "\n",
        "> HINT: Check out the [docs](https://platform.openai.com/docs/guides/embeddings) to help you answer this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer:  length of the embedding vector will be 1536 by default. but can be reduced using dimensions parameter. \n",
        "https://openai.com/blog/new-embedding-models-and-api-updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIl5RQ98qFVu"
      },
      "source": [
        "Now, let's see how \"puppy\" and \"dog\" are related to eachother!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVn0WjMNqJrD",
        "outputId": "fb331f4c-b942-4862-aec7-709edf70c027"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5590390640733375"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_vec, dog_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfozbFmTielw"
      },
      "source": [
        "We can repeat the experiment for things we might expect to be unrelated, as well:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "8PO-ONaJikDX"
      },
      "outputs": [],
      "source": [
        "puppy_vec = embedding_model.embed_query(\"puppy\")\n",
        "ice_vec = embedding_model.embed_query(\"ice cube\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-BYB5-WipM0",
        "outputId": "12372d9a-982a-470b-dd61-fa5dc4bf113d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.20365601127332925"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_vec, ice_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-AweaFtisKS"
      },
      "source": [
        "As expected, we get an unrelated score!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG9MnpJSsSUg"
      },
      "source": [
        "Great!\n",
        "\n",
        "Now, let's extend it to our example.\n",
        "\n",
        "What we want to do is find the most related phrases to our query - so what we need to do is find the dense continuous vector representations for each of the chunks in our courpus - and then compare them against the dense continuous vector representations of our query.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "Compare the embedding of our query with the embeddings of each of our chunks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByK-zb0FsnqR"
      },
      "source": [
        "### Finding the Embeddings for Our Chunks\n",
        "\n",
        "First, let's find all our embeddings for each chunk and store them in a convenient format for later. Here we are storing them in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "ZHl-u6Fxske9"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ9wY2etK7y",
        "outputId": "ab4b35c9-5a70-477d-96fc-49bf8ee8e11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk - LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n",
            "Chunk - Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
            "---\n",
            "Embedding - Vector of Size: 1536\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for k,v in embeddings_dict.items():\n",
        "  print(f\"Chunk - {k}\")\n",
        "  print(\"---\")\n",
        "  print(f\"Embedding - Vector of Size: {len(v)}\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOxYybdNtkWv"
      },
      "source": [
        "Okay, great. Let's create a query - and then embed it! Query vector store by sending it to openai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQD5Zwl1tLrZ",
        "outputId": "bfa56ab9-ee1d-4748-b806-a43a92c33ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector of Size: 1536\n"
          ]
        }
      ],
      "source": [
        "query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "query_vector = embedding_model.embed_query(query)\n",
        "print(f\"Vector of Size: {len(query_vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJhyvgEt5Vt"
      },
      "source": [
        "Now, let's compare it against each existing chunk's embedding by using cosine similarity.\n",
        "Here we get the closest chunk to our query. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwnJ0uYQt-G_",
        "outputId": "d2971742-4f79-4abe-c20b-b40701d69d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
            "0.5372562043448952\n"
          ]
        }
      ],
      "source": [
        "max_similarity = -float('inf')\n",
        "closest_chunk = \"\"\n",
        "\n",
        "for chunk, chunk_vector in embeddings_dict.items():\n",
        "  cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "  if cosine_similarity_score > max_similarity:\n",
        "    closest_chunk = chunk\n",
        "    max_similarity = cosine_similarity_score\n",
        "\n",
        "print(closest_chunk)\n",
        "print(max_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDC7HS7iumN-"
      },
      "source": [
        "And we get the expected result, which is the passage that specifically mentions prototyping in a Jupyter Notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPJexL1kuw45"
      },
      "source": [
        "### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "Here, we'll wrap the above code in a helper function callled retrieve_context!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "tnLpo26pu8-1"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query, embeddings_dict, embedding_model):\n",
        "  query_vector = embedding_model.embed_query(query)\n",
        "  max_similarity = -float('inf')\n",
        "  closest_chunk = \"\"\n",
        "\n",
        "  for chunk, chunk_vector in embeddings_dict.items():\n",
        "    cosine_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "    if cosine_similarity_score > max_similarity:\n",
        "      closest_chunk = chunk\n",
        "      max_similarity = cosine_similarity_score\n",
        "\n",
        "  return closest_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrINkVrvL1Q"
      },
      "source": [
        "Now, let's add it to our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "Q26pl1osvNmL"
      },
      "outputs": [],
      "source": [
        "def simple_rag(query, embeddings_dict, embedding_model, chat_chain):\n",
        "  context = retrieve_context(query, embeddings_dict, embedding_model)\n",
        "\n",
        "  response = chat_chain.invoke({\"query\" : query, \"context\" : context})\n",
        "\n",
        "  return_package = {\n",
        "      \"query\" : query,\n",
        "      \"response\" : response,\n",
        "      \"retriever_context\" : context\n",
        "  }\n",
        "\n",
        "  return return_package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHjbWxTAvtLS",
        "outputId": "6801e6ba-8413-4a86-c736-9a359163081f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'Can LCEL help take code from the notebook to production?',\n",
              " 'response': AIMessage(content='Yes, LCEL can help take code from the notebook to production by providing full sync, async, batch, and streaming support for chains constructed in this way. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface and then seamlessly expose it as an async streaming interface in a production environment.', response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 152, 'total_tokens': 216}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2f43950d-bfab-41cb-ba28-d10e9146afc5-0'),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " # here we run the function to get our response\n",
        "simple_rag(\"Can LCEL help take code from the notebook to production?\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD2URVX3O3XJ"
      },
      "source": [
        "####â“ Question #3:\n",
        "\n",
        "What does LCEL do that makes it more reliable at scale?\n",
        "\n",
        "> HINT: Use your newly created `simple_rag` to help you answer this question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What does LCEL do that makes it more reliable at scale?',\n",
              " 'response': AIMessage(content='LCEL provides full sync, async, batch, and streaming support for any chain constructed using this language. This makes it more reliable at scale because it can easily handle different types of operations and data processing efficiently.', response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 153, 'total_tokens': 195}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-9ce8d823-a4aa-4747-9bbb-7bba36ae8959-0'),\n",
              " 'retriever_context': 'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'}"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_rag(\"What does LCEL do that makes it more reliable at scale?\", embeddings_dict, embedding_model, chat_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfYsBu45-0pL"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #3: Create a Simple RAG Application Using Qdrant, OpenAI, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and OpenAI to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using Drant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "Let's use [The Ultimate Hitchhiker's Guide](https://jaydixit.com/files/PDFs/TheultimateHitchhikersGuide.pdf) as our data today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `PyMUPDFLoader` to load our PDF directly from the web!\n",
        "\n",
        "Here we import pdf loader and load our document to docs variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "-AHA9L3Jxo3r"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = PyMuPDFLoader(\"https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "eec7df70-56d4-433a-a870-ad143c6f0a6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "517"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "91f274c3-1117-43ad-f6af-03bf4974e776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "189\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!\n",
        "\n",
        "Here using community chain vectorstores and importing Qdrant to store our embeddings. Using in memory vector store - no need to create an index.\n",
        "call collection hitchikers guide. this will load all docs and get embeddings. Runs pretty quickly as opposed to the local dictionary function - the olocal one I gave up at 15mins. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Hitchiker's Guide\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know the answer to that question based on the provided context.\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "9a220163-05bb-4cb0-a620-66ad6fe1d04c"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "9e51c77e-e4d1-41f0-afd8-c6f3d598c61d"
      },
      "outputs": [],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In Douglas Adam's Hitchhicker's Guide, towels are portrayed as incredibly useful and important items. The book mentions that a towel is the most massively useful thing a hitchhiker can have, as it can be used for various purposes such as signaling for help, drying off, and even has immense psychological value. The presence of a towel is seen as a sign of a prepared and resourceful individual, and possessing one can lead others to assume that the person also has other necessary items like a toothbrush, soap, and more. Towels have become a symbol of readiness and capability in the world of hitchhiking in the book.\""
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSZFdCM5LFoq",
        "outputId": "9a853f7c-65d8-4772-e54c-d3e6a2ccf526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content=\"28  /  D O U G L A S  A D A M S  \\nthis device was in fact that most remarkable of all books ever to \\ncome out of the great publishing corporations of Ursa Minor - \\nThe Hitch Hiker's Guide to the Galaxy.  The reason why it was \\npublished in the form of a micro sub meson electronic \\ncomponent is that if it were printed in normal book form, an \\ninterstellar hitch hiker would require several inconveniently \\nlarge buildings to carry it around in. \\nBeneath that in Ford Prefect's satchel were a few biros, a \\nnotepad, and a largish bath towel from Marks and Spencer. \\nThe Hitch Hiker's Guide to the Galaxy has a few things to say on \\nthe subject of towels. \\nA towel, it says, is about the most massively useful thing an\" metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 27, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': 'b98ba3fcb3a54dc6b85a9b8f22721144', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content=\"you can't see it, it can't see you - daft as a bush, but very \\nravenous); you can wave your towel in emergencies as a distress \\nsignal, and of course dry yourself off with it if it still seems to be \\nclean enough. \\nMore importantly, a towel has immense psychological value.  For \\nsome reason, if a strag (strag: non-hitch hiker) discovers that a \\nhitch hiker has his towel with him, he will automatically assume \\nthat he is also in possession of a toothbrush, face flannel, soap, tin\" metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 27, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': '3bfe42e32d634376a26d4e465531cc8b', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content='T H E  H I T C H H I K E R \\' S  G U I D E  T O  T H E  G A L A X Y  /  29  \\nof biscuits, flask, compass, map, ball of string, gnat spray, wet \\nweather gear, space suit etc., etc.  Furthermore, the strag will \\nthen happily lend the hitch hiker any of these or a dozen other \\nitems that the hitch hiker might accidentally have \"lost\".  What \\nthe strag will think is that any man who can hitch the length and \\nbreadth of the galaxy, rough it, slum it, struggle against terrible \\nodds, win through, and still knows where his towel is is clearly a \\nman to be reckoned with. \\nHence a phrase which has passed into hitch hiking slang, as in' metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 28, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': '24bc341b698840048caed28936369ae2', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n",
            "Context:\n",
            "page_content='\"Hey, you sass that hoopy Ford Prefect? There\\'s a frood who \\nreally knows where his towel is.\" (Sass: know, be aware of, meet, \\nhave sex with; hoopy: really together guy; frood: really amazingly \\ntogether guy.) \\nNestling quietly on top of the towel in Ford Prefect\\'s satchel, the \\nSub-Etha Sens-O-Matic began to wink more quickly.  Miles above \\nthe surface of the planet the huge yellow somethings began to fan \\nout.  At Jodrell Bank, someone decided it was time for a nice \\nrelaxing cup of tea. \\n\"You got a towel with you?\" said Ford Prefect suddenly to Arthur. \\nArthur, struggling through his third pint, looked round at him. \\n\"Why? What, no ...  should I have?\" He had given up being' metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 28, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4', '_id': 'fddcda8bedaa43c0a56ae8a6230c835c', '_collection_name': \"Hitchiker's Guide\"}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "a68a9a96-28de-408f-ba6b-935c5ee46c44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know\""
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know\""
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"Did Fenerbahce beat Olympiakos during Champions league quarter finals?\"})\n",
        "response[\"response\"].content\n",
        "\n",
        "# Not sure why I cant get it to respond: \"I don't know the answer to that question based on the provided context.\" maybe because my initial prompt was with \"I dont know\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvkSiGXrPMYw"
      },
      "source": [
        "####â“ Question #4:\n",
        "\n",
        "Where does Arthur Dent meet Marvin? - Yes.\n",
        "\n",
        "> HINT: Use your RAG Chain to answer this question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Arthur Dent meets Marvin in a corridor where Marvin is trudging along and moaning about the pain in his diodes. Arthur walks along beside him, listening to Marvin's complaints.\""
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"Where does Arthur Dent meet Marvin?\"})\n",
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pinecone Implemenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet  langchain-pinecone langchain-openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create an index - serverless\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "#pc = Pinecone(api_key=\"YOUR_API_KEY\")\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "pc.create_index(\n",
        "  name=\"hitchikers-guidev2\", #name of the index. There are some rules such as lower case, no special chars, etc\n",
        "  dimension=1536,\n",
        "  metric=\"cosine\",  #chose the basic cosine similarity since I want to compare with Qrandt\n",
        "  spec=ServerlessSpec(\n",
        "    cloud=\"aws\",\n",
        "    region=\"us-east-1\"\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "index_name = \"hitchikers-guidev2\"\n",
        "namespace = \"serverless\"\n",
        "vectorstore = Pinecone(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings,\n",
        "    namespace=namespace,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "loader = PyMuPDFLoader(\"https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf\")\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'': {'vector_count': 227}},\n",
            " 'total_vector_count': 227}\n"
          ]
        }
      ],
      "source": [
        "index_name = \"hitchikers-guidev2\"\n",
        "index = pc.Index(index_name)\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "index_name = \"hitchikers-guidev2\"\n",
        "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28  /  D O U G L A S  A D A M S  \n",
            "this device was in fact that most remarkable of all books ever to \n",
            "come out of the great publishing corporations of Ursa Minor - \n",
            "The Hitch Hiker's Guide to the Galaxy.  The reason why it was \n",
            "published in the form of a micro sub meson electronic \n",
            "component is that if it were printed in normal book form, an \n",
            "interstellar hitch hiker would require several inconveniently \n",
            "large buildings to carry it around in. \n",
            "Beneath that in Ford Prefect's satchel were a few biros, a \n",
            "notepad, and a largish bath towel from Marks and Spencer. \n",
            "The Hitch Hiker's Guide to the Galaxy has a few things to say on \n",
            "the subject of towels. \n",
            "A towel, it says, is about the most massively useful thing an \n",
            "interstellar hitch hiker can have.  Partly it has great practical \n",
            "value - you can wrap it around you for warmth as you bound \n",
            "across the cold moons of Jaglan Beta; you can lie on it on the \n",
            "brilliant marble-sanded beaches of Santraginus V, inhaling the \n",
            "heady sea vapours; you can sleep under it beneath the stars \n",
            "which shine so redly on the desert world of Kakrafoon; use it to \n",
            "sail a mini raft down the slow heavy river Moth; wet it for use in \n",
            "hand-to- hand-combat; wrap it round your head to ward off \n",
            "noxious fumes or to avoid the gaze of the Ravenous Bugblatter \n",
            "Beast of Traal (a mindboggingly stupid animal, it assumes that if \n",
            "you can't see it, it can't see you - daft as a bush, but very \n",
            "ravenous); you can wave your towel in emergencies as a distress \n",
            "signal, and of course dry yourself off with it if it still seems to be \n",
            "clean enough. \n",
            "More importantly, a towel has immense psychological value.  For \n",
            "some reason, if a strag (strag: non-hitch hiker) discovers that a \n",
            "hitch hiker has his towel with him, he will automatically assume \n",
            "that he is also in possession of a toothbrush, face flannel, soap, tin\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Maximal Marginal Relevance Search (MMR) as retriever (Adjust balance between relevance and diversity)\n",
        "Maximal Marginal Relevance (MMR) is a method used in information retrieval systems to create a balanced trade-off between relevance and diversity in the results returned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Document 0\n",
            "\n",
            "28  /  D O U G L A S  A D A M S  \n",
            "this device was in fact that most remarkable of all books ever to \n",
            "come out of the great publishing corporations of Ursa Minor - \n",
            "The Hitch Hiker's Guide to the Galaxy.  The reason why it was \n",
            "published in the form of a micro sub meson electronic \n",
            "component is that if it were printed in normal book form, an \n",
            "interstellar hitch hiker would require several inconveniently \n",
            "large buildings to carry it around in. \n",
            "Beneath that in Ford Prefect's satchel were a few biros, a \n",
            "notepad, and a largish bath towel from Marks and Spencer. \n",
            "The Hitch Hiker's Guide to the Galaxy has a few things to say on \n",
            "the subject of towels. \n",
            "A towel, it says, is about the most massively useful thing an \n",
            "interstellar hitch hiker can have.  Partly it has great practical \n",
            "value - you can wrap it around you for warmth as you bound \n",
            "across the cold moons of Jaglan Beta; you can lie on it on the \n",
            "brilliant marble-sanded beaches of Santraginus V, inhaling the \n",
            "heady sea vapours; you can sleep under it beneath the stars \n",
            "which shine so redly on the desert world of Kakrafoon; use it to \n",
            "sail a mini raft down the slow heavy river Moth; wet it for use in \n",
            "hand-to- hand-combat; wrap it round your head to ward off \n",
            "noxious fumes or to avoid the gaze of the Ravenous Bugblatter \n",
            "Beast of Traal (a mindboggingly stupid animal, it assumes that if \n",
            "you can't see it, it can't see you - daft as a bush, but very \n",
            "ravenous); you can wave your towel in emergencies as a distress \n",
            "signal, and of course dry yourself off with it if it still seems to be \n",
            "clean enough. \n",
            "More importantly, a towel has immense psychological value.  For \n",
            "some reason, if a strag (strag: non-hitch hiker) discovers that a \n",
            "hitch hiker has his towel with him, he will automatically assume \n",
            "that he is also in possession of a toothbrush, face flannel, soap, tin\n",
            "\n",
            "## Document 1\n",
            "\n",
            "14  /  D O U G L A S  A D A M S  \n",
            "Ford wished that a flying saucer would arrive soon because he \n",
            "knew how to flag flying saucers down and get lifts from them.  \n",
            "He knew how to see the Marvels of the Universe for less than \n",
            "thirty Altairan dollars a day. \n",
            "In fact, Ford Prefect was a roving researcher for that wholly \n",
            "remarkable book The Hitch Hiker's Guide to the Galaxy. \n",
            "Human beings are great adaptors, and by lunchtime life in the \n",
            "environs of Arthur's house had settled into a steady routine.  It \n",
            "was Arthur's accepted role to lie squelching in the mud making \n",
            "occasional demands to see his lawyer, his mother or a good book; \n",
            "it was Mr Prosser's accepted role to tackle Arthur with the \n",
            "occasional new ploy such as the For the Public Good talk, the \n",
            "March of Progress talk, the They Knocked My House Down \n",
            "Once You Know, Never Looked Back talk and various other \n",
            "cajoleries and threats; and it was the bulldozer drivers' accepted \n",
            "role to sit around drinking coffee and experimenting with union \n",
            "regulations to see how they could turn the situation to their \n",
            "financial advantage. \n",
            "The Earth moved slowly in its diurnal course. \n",
            "The sun was beginning to dry out the mud Arthur lay in. \n",
            "A shadow moved across him again. \n",
            "\"Hello Arthur,\" said the shadow.  Arthur looked up and squinting \n",
            "into the sun was startled to see Ford Prefect standing above him. \n",
            "\"Ford! Hello, how are you?\" \n",
            "\"Fine,\" said Ford, \"look, are you busy?\"\n",
            "\n",
            "## Document 2\n",
            "\n",
            "THE HITCHHIKER'S GUIDE TO THE GALAXY \n",
            "B Y  D O U G L A S  A D A M S  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "2 0 0 1  H A N O M A G  \n",
            "D O C U M E N T  V E R S I O N  1 . 0\n",
            "\n",
            "## Document 3\n",
            "\n",
            "T H E  H I T C H H I K E R ' S  G U I D E  T O  T H E  G A L A X Y  /  29  \n",
            "of biscuits, flask, compass, map, ball of string, gnat spray, wet \n",
            "weather gear, space suit etc., etc.  Furthermore, the strag will \n",
            "then happily lend the hitch hiker any of these or a dozen other \n",
            "items that the hitch hiker might accidentally have \"lost\".  What \n",
            "the strag will think is that any man who can hitch the length and \n",
            "breadth of the galaxy, rough it, slum it, struggle against terrible \n",
            "odds, win through, and still knows where his towel is is clearly a \n",
            "man to be reckoned with. \n",
            "Hence a phrase which has passed into hitch hiking slang, as in \n",
            "\"Hey, you sass that hoopy Ford Prefect? There's a frood who \n",
            "really knows where his towel is.\" (Sass: know, be aware of, meet, \n",
            "have sex with; hoopy: really together guy; frood: really amazingly \n",
            "together guy.) \n",
            "Nestling quietly on top of the towel in Ford Prefect's satchel, the \n",
            "Sub-Etha Sens-O-Matic began to wink more quickly.  Miles above \n",
            "the surface of the planet the huge yellow somethings began to fan \n",
            "out.  At Jodrell Bank, someone decided it was time for a nice \n",
            "relaxing cup of tea. \n",
            "\"You got a towel with you?\" said Ford Prefect suddenly to Arthur. \n",
            "Arthur, struggling through his third pint, looked round at him. \n",
            "\"Why? What, no ...  should I have?\" He had given up being \n",
            "surprised, there didn't seem to be any point any longer. \n",
            "Ford clicked his tongue in irritation. \n",
            "\"Drink up,\" he urged. \n",
            "At that moment the dull sound of a rumbling crash from outside \n",
            "filtered through the low murmur of the pub, through the sound\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"\n",
        "retriever = docsearch.as_retriever(search_type=\"mmr\")\n",
        "matched_docs = retriever.get_relevant_documents(query)\n",
        "for i, d in enumerate(matched_docs):\n",
        "    print(f\"\\n## Document {i}\\n\")\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"28  /  D O U G L A S  A D A M S  \\nthis device was in fact that most remarkable of all books ever to \\ncome out of the great publishing corporations of Ursa Minor - \\nThe Hitch Hiker's Guide to the Galaxy.  The reason why it was \\npublished in the form of a micro sub meson electronic \\ncomponent is that if it were printed in normal book form, an \\ninterstellar hitch hiker would require several inconveniently \\nlarge buildings to carry it around in. \\nBeneath that in Ford Prefect's satchel were a few biros, a \\nnotepad, and a largish bath towel from Marks and Spencer. \\nThe Hitch Hiker's Guide to the Galaxy has a few things to say on \\nthe subject of towels. \\nA towel, it says, is about the most massively useful thing an \\ninterstellar hitch hiker can have.  Partly it has great practical \\nvalue - you can wrap it around you for warmth as you bound \\nacross the cold moons of Jaglan Beta; you can lie on it on the \\nbrilliant marble-sanded beaches of Santraginus V, inhaling the \\nheady sea vapours; you can sleep under it beneath the stars \\nwhich shine so redly on the desert world of Kakrafoon; use it to \\nsail a mini raft down the slow heavy river Moth; wet it for use in \\nhand-to- hand-combat; wrap it round your head to ward off \\nnoxious fumes or to avoid the gaze of the Ravenous Bugblatter \\nBeast of Traal (a mindboggingly stupid animal, it assumes that if \\nyou can't see it, it can't see you - daft as a bush, but very \\nravenous); you can wave your towel in emergencies as a distress \\nsignal, and of course dry yourself off with it if it still seems to be \\nclean enough. \\nMore importantly, a towel has immense psychological value.  For \\nsome reason, if a strag (strag: non-hitch hiker) discovers that a \\nhitch hiker has his towel with him, he will automatically assume \\nthat he is also in possession of a toothbrush, face flannel, soap, tin\", metadata={'author': 'Douglas Adams', 'creationDate': 'D:20010213123949', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'encryption': 'Standard V1 R2 40-bit RC4', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'format': 'PDF 1.3', 'keywords': 'hanomag <01name@iname.com>', 'modDate': \"D:20010213124359-08'00'\", 'page': 27.0, 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'subject': 'document version 1.0', 'title': \"Hitchhiker's Guide to the Galaxy\", 'total_pages': 227.0, 'trapped': ''}),\n",
              " Document(page_content='T H E  H I T C H H I K E R \\' S  G U I D E  T O  T H E  G A L A X Y  /  29  \\nof biscuits, flask, compass, map, ball of string, gnat spray, wet \\nweather gear, space suit etc., etc.  Furthermore, the strag will \\nthen happily lend the hitch hiker any of these or a dozen other \\nitems that the hitch hiker might accidentally have \"lost\".  What \\nthe strag will think is that any man who can hitch the length and \\nbreadth of the galaxy, rough it, slum it, struggle against terrible \\nodds, win through, and still knows where his towel is is clearly a \\nman to be reckoned with. \\nHence a phrase which has passed into hitch hiking slang, as in \\n\"Hey, you sass that hoopy Ford Prefect? There\\'s a frood who \\nreally knows where his towel is.\" (Sass: know, be aware of, meet, \\nhave sex with; hoopy: really together guy; frood: really amazingly \\ntogether guy.) \\nNestling quietly on top of the towel in Ford Prefect\\'s satchel, the \\nSub-Etha Sens-O-Matic began to wink more quickly.  Miles above \\nthe surface of the planet the huge yellow somethings began to fan \\nout.  At Jodrell Bank, someone decided it was time for a nice \\nrelaxing cup of tea. \\n\"You got a towel with you?\" said Ford Prefect suddenly to Arthur. \\nArthur, struggling through his third pint, looked round at him. \\n\"Why? What, no ...  should I have?\" He had given up being \\nsurprised, there didn\\'t seem to be any point any longer. \\nFord clicked his tongue in irritation. \\n\"Drink up,\" he urged. \\nAt that moment the dull sound of a rumbling crash from outside \\nfiltered through the low murmur of the pub, through the sound', metadata={'author': 'Douglas Adams', 'creationDate': 'D:20010213123949', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'encryption': 'Standard V1 R2 40-bit RC4', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'format': 'PDF 1.3', 'keywords': 'hanomag <01name@iname.com>', 'modDate': \"D:20010213124359-08'00'\", 'page': 28.0, 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'subject': 'document version 1.0', 'title': \"Hitchhiker's Guide to the Galaxy\", 'total_pages': 227.0, 'trapped': ''}),\n",
              " Document(page_content=\"154  / D O U G L A S  A D A M S  \\nOn the surface of Magrathea Arthur wandered about moodily. \\nFord had thoughtfully left him his copy of The Hitch Hiker's \\nGuide to the Galaxy to while away the time with.  He pushed a \\nfew buttons at random. \\nThe Hitch Hiker's Guide to the Galaxy is a very unevenly edited \\nbook and contains many passages that simply seemed to its \\neditors like a good idea at the time. \\nOne of these (the one Arthur now came across) supposedly \\nrelates the experiences of one Veet Voojagig, a quiet young \\nstudent at the University of Maximegalon, who pursued a \\nbrilliant \\nacademic \\ncareer \\nstudying \\nancient \\nphilology, \\ntransformational ethics and the wave harmonic theory of \\nhistorical perception, and then, after a night of drinking Pan \\nGalactic Gargle Blasters with Zaphod Beeblebrox, became \\nincreasingly obsessed with the problem of what had happened to \\nall the biros he'd bought over the past few years. \\nThere followed a long period of painstaking research during \\nwhich he visited all the major centres of biro loss throughout the \\ngalaxy and eventually came up with a quaint little theory which \\nquite caught the public imagination at the time.  Somewhere in \\nthe cosmos, he said, along with all the planets inhabited by \\nhumanoids, \\nreptiloids, \\nfishoids, \\nwalking \\ntreeoids \\nand \\nsuperintelligent shades of the colour blue, there was also a planet \\nentirely given over to biro life forms.  And it was to this planet \\nthat unattended biros would make their way, slipping away \\nquietly through wormholes in space to a world where they knew \\nthey could enjoy a uniquely biroid lifestyle, responding to highly \\nbiro-oriented stimuli, and generally leading the biro equivalent \\nof the good life.\", metadata={'author': 'Douglas Adams', 'creationDate': 'D:20010213123949', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'encryption': 'Standard V1 R2 40-bit RC4', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'format': 'PDF 1.3', 'keywords': 'hanomag <01name@iname.com>', 'modDate': \"D:20010213124359-08'00'\", 'page': 153.0, 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'subject': 'document version 1.0', 'title': \"Hitchhiker's Guide to the Galaxy\", 'total_pages': 227.0, 'trapped': ''}),\n",
              " Document(page_content=\"4  /  D O U G L A S  A D A M S  \\nSadly, however, before she could get to a phone to tell anyone- \\nabout it, a terribly stupid catastrophe occurred, and the idea was \\nlost forever. \\nThis is not her story. \\nBut it is the story of that terrible stupid catastrophe and some of \\nits consequences. \\nIt is also the story of a book, a book called The Hitch Hiker's \\nGuide to the Galaxy - not an Earth book, never published on \\nEarth, and until the terrible catastrophe occurred, never seen or \\nheard of by any Earthman. \\nNevertheless, a wholly remarkable book. \\nin fact it was probably the most remarkable book ever to come \\nout of the great publishing houses of Ursa Minor - of which no \\nEarthman had ever heard either. \\nNot only is it a wholly remarkable book, it is also a highly \\nsuccessful one - more popular than the Celestial Home Care \\nOmnibus, better selling than Fifty More Things to do in Zero \\nGravity, and more controversial than Oolon Colluphid's trilogy of \\nphilosophical blockbusters Where God Went Wrong, Some More \\nof God's Greatest Mistakes and Who is this God Person Anyway? \\nIn many of the more relaxed civilizations on the Outer Eastern \\nRim of the Galaxy, the Hitch Hiker's Guide has already \\nsupplanted the great Encyclopedia Galactica as the standard \\nrepository of all knowledge and wisdom, for though it has many \\nomissions and contains much that is apocryphal, or at least wildly \\ninaccurate, it scores over the older, more pedestrian work in two \\nimportant respects.\", metadata={'author': 'Douglas Adams', 'creationDate': 'D:20010213123949', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'encryption': 'Standard V1 R2 40-bit RC4', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'format': 'PDF 1.3', 'keywords': 'hanomag <01name@iname.com>', 'modDate': \"D:20010213124359-08'00'\", 'page': 3.0, 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'subject': 'document version 1.0', 'title': \"Hitchhiker's Guide to the Galaxy\", 'total_pages': 227.0, 'trapped': ''})]"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore = PineconeVectorStore(index_name='hitchikers-guidev2', embedding=embeddings)\n",
        "query = \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"\n",
        "vectorstore.similarity_search(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"28  /  D O U G L A S  A D A M S  \\nthis device was in fact that most remarkable of all books ever to \\ncome out of the great publishing corporations of Ursa Minor - \\nThe Hitch Hiker's Guide to the Galaxy.  The reason why it was \\npublished in the form of a micro sub meson electronic \\ncomponent is that if it were printed in normal book form, an \\ninterstellar hitch hiker would require several inconveniently \\nlarge buildings to carry it around in. \\nBeneath that in Ford Prefect's satchel were a few biros, a \\nnotepad, and a largish bath towel from Marks and Spencer. \\nThe Hitch Hiker's Guide to the Galaxy has a few things to say on \\nthe subject of towels. \\nA towel, it says, is about the most massively useful thing an \\ninterstellar hitch hiker can have.  Partly it has great practical \\nvalue - you can wrap it around you for warmth as you bound \\nacross the cold moons of Jaglan Beta; you can lie on it on the \\nbrilliant marble-sanded beaches of Santraginus V, inhaling the \\nheady sea vapours; you can sleep under it beneath the stars \\nwhich shine so redly on the desert world of Kakrafoon; use it to \\nsail a mini raft down the slow heavy river Moth; wet it for use in \\nhand-to- hand-combat; wrap it round your head to ward off \\nnoxious fumes or to avoid the gaze of the Ravenous Bugblatter \\nBeast of Traal (a mindboggingly stupid animal, it assumes that if \\nyou can't see it, it can't see you - daft as a bush, but very \\nravenous); you can wave your towel in emergencies as a distress \\nsignal, and of course dry yourself off with it if it still seems to be \\nclean enough. \\nMore importantly, a towel has immense psychological value.  For \\nsome reason, if a strag (strag: non-hitch hiker) discovers that a \\nhitch hiker has his towel with him, he will automatically assume \\nthat he is also in possession of a toothbrush, face flannel, soap, tin\", metadata={'author': 'Douglas Adams', 'creationDate': 'D:20010213123949', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'encryption': 'Standard V1 R2 40-bit RC4', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'format': 'PDF 1.3', 'keywords': 'hanomag <01name@iname.com>', 'modDate': \"D:20010213124359-08'00'\", 'page': 27.0, 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'subject': 'document version 1.0', 'title': \"Hitchhiker's Guide to the Galaxy\", 'total_pages': 227.0, 'trapped': ''})]"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore.similarity_search(  \n",
        "    query,  # our search query  \n",
        "    k=1  # return top most relevant doc \n",
        ") "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain_env",
      "language": "python",
      "name": "langchain_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
