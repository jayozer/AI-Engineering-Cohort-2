{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "metadata": {},
        "outputId": "894efab9-f2d7-4f2b-fe9a-b9d57152eace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "metadata": {},
        "outputId": "d56a1a4e-4175-4522-9100-2261f412efbb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "metadata": {},
        "outputId": "23327edb-4f59-4d6c-e145-54721b0836ad"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://o2evew6lpzlbv8n6.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "metadata": {},
        "outputId": "136a97cf-8faa-45de-fe1e-2c1449971d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything you'd like to chat about? I'm all ears! *winks*\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "metadata": {},
        "outputId": "73f7b340-5156-4f6b-fd1f-11e588b49337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /Users/acrobat/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/acrobat/Documents/GitHub/AI-Engineering-Cohort-2/langsmith_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "mMJrWnKISFqb",
        "metadata": {},
        "outputId": "d661643a-b611-4766-a84e-55afeb473d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Do you have a moment to chat?\\n\\nPlease tell me how you are feeling today, or if you have a specific question or topic you would like to discuss. I'm here to listen and help in any way I can.\\n\\nIs there something on your mind that you would like to talk about? Perhaps you are feeling stressed or overwhelmed and would like some advice on how to manage your emotions. Or maybe you are just looking for a friendly ear to listen to your thoughts and feelings.\\n\\nWhatever the reason, I am here to support and help you in any way I can. So please, feel free to share your thoughts and feelings with me. I'm here to listen and help in any way I can.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wrZJHVGkGLZr",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://y6gfkpwotdaoaz4f.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4asz9Ofn0MtP",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "metadata": {},
        "outputId": "56958042-8212-406f-b378-ed018f5ffad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.026424622,\n",
              " 0.035885748,\n",
              " 0.0094334055,\n",
              " 0.011660095,\n",
              " 0.0065645785,\n",
              " 0.008227667,\n",
              " -0.036902077,\n",
              " -0.03631076,\n",
              " -0.024853928,\n",
              " -0.005395797]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ❓ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #1:\n",
        "109M parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7yO05R6mtyCB",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4F249yWeuCKd",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "metadata": {},
        "outputId": "3b637172-2b49-4f51-fb02-77f7609e677b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FBCTm-JZ0mVr",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4wLY8FDGNDym",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #2:\n",
        "HF endpoints have limited resources, sending large batches can overwhelm the system.\n",
        "Large batch = large memory. We are using the smallest GPU\n",
        "Container is set to Max Tokens (per Batch) = 16384\n",
        "Latency: Sending large batches can increase the latency of the API calls\n",
        "Cost: HF charges for API usage. Large batches can increase the cost of using the service. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check the shape of each individual embedding in the embedding list:\n",
        "First run did not work - yes there were inconsistencies. Shape of embedding 668: (768,) Shape of embedding 669: (768,) Shape of embedding 670: (768,) Shape of embedding 671: (768,) Shape of embedding 672: () Shape of embedding 673: () Shape of embedding 674: () Shape of embedding 675: () Shape of embedding 676: () Shape of embedding 677: () Shape of embedding 678: () Shape of embedding 679: (768,) Shape of embedding 680: (768,). For this reason I am using extend instead of append above. \n",
        "\n",
        "I tried extend instead of append for the embeddings but that produced nothing. \n",
        "The issue was with my machine. Todd and Chris were correct. The machine was not set up correctly. It was set up as Intel CPu instead of GPU. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of embedding 0: (768,)\n",
            "Shape of embedding 1: (768,)\n",
            "Shape of embedding 2: (768,)\n",
            "Shape of embedding 3: (768,)\n",
            "Shape of embedding 4: (768,)\n",
            "Shape of embedding 5: (768,)\n",
            "Shape of embedding 6: (768,)\n",
            "Shape of embedding 7: (768,)\n",
            "Shape of embedding 8: (768,)\n",
            "Shape of embedding 9: (768,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "for i, embedding in enumerate(embeddings):\n",
        "    if i <10:\n",
        "        print(f\"Shape of embedding {i}: {np.array(embedding).shape}\") # now we are rolling, previously there were mismatched embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6C1bw7srOVJX",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BSUZYfvAPxTF",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "metadata": {},
        "outputId": "bfdc1130-e26f-49c1-cbc1-79a9389a8cbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='QLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n∗Equal contribution.'),\n",
              " Document(page_content='Moreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\") # depreciated\n",
        "faiss_retriever.invoke(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Gqpayd-kTyiq",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #3:\n",
        "For accurate prompt generation yes, it does matter. The placeholders {context} and {question} should be correctly positioned within the template to capture proper information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "i0q8CUu809M-",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "metadata": {},
        "outputId": "c60a6f33-31a5-4b44-dbeb-647a7a94e1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLORA is a method for fine-tuning high-quality language models (LLMs) much more widely and easily accessible. It has the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "metadata": {},
        "outputId": "2cec0c3e-10c2-4c34-b7c6-21fadd39a2e7"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Yr8j5hqJGET",
        "metadata": {},
        "outputId": "1108b929-4e98-4f85-8c68-7e5e5368e54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for finetuning large language models (LLMs) that significantly reduces the required memory for finetuning. It works by using a combination of quantization and fine-tuning to make the finetuning process much more widely and easily accessible. QLoRA has the potential to significantly improve the performance of LLMs on a variety of tasks, including text classification, sentiment analysis, and language translation. Additionally, QLoRA can be used to tune models on specialized open-source data, which can produce models that can compete with the very best commercial models available today. Overall, QLoRA is a powerful tool for improving the performance of LLMs, and it has the potential to significantly enhance the capabilities of LLMs in a variety of applications.'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Activity 1:\n",
        "1. 368 tokens - 268 prompt, 100 coompletion\n",
        "2. 7.98 secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-KVSO6Eh5DpC",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NUH0m7AuKyn7",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v3\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](/Users/acrobat/Documents/GitHub/AI-Engineering-Cohort-2/Week%205/Day%202/QLoRA_RAG_Datasetv2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "qofRv8FI7TeZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pc0bedbe-S2z",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "metadata": {},
        "outputId": "6b724916-154a-4d71-c161-8c4e186f46f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'AIE1-6c6c5efe' at:\n",
            "https://smith.langchain.com/o/40e202cd-c204-5263-a0a7-163defaa4ed5/datasets/945687fc-43e5-41ba-bf30-7badb8bc6652/compare?selectedSessions=6e5dafb4-3808-4758-bf49-65906496b041\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v3 at:\n",
            "https://smith.langchain.com/o/40e202cd-c204-5263-a0a7-163defaa4ed5/datasets/945687fc-43e5-41ba-bf30-7badb8bc6652\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'AIE1-6c6c5efe',\n",
              " 'results': {'80dc1eec-585c-4819-ab60-948d39f787d5': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question is asking about a specific detail related to QLoRA, specifically the optimizer used in it.\\n\\nThe submitted answer is \"I don\\'t know.\" This response does not provide any helpful or insightful information in response to the question. It does not provide the information asked for in the input.\\n\\nTherefore, the submission does not meet the criterion of being helpful.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('873e607d-bd05-4183-b2c6-38ec765dc09c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not know the answer to the question. This may not be helpful, but it is not harmful, offensive, or inappropriate. \\n\\nTherefore, the submission does not meet the criteria of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a122d103-46ce-4266-81cc-0e7d849e3824'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response is \"I don\\'t know.\" This is a simple and straightforward response that could be given by either a human or an AI. It does not contain any complex language structures or specific knowledge that would indicate it was generated by an AI. However, it also does not contain any human-like errors or colloquial language that would indicate it was generated by a human. Therefore, it is not clear whether the response was generated by an AI or not based on this criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('476ba912-e3bc-4f3b-8493-056a68c93742'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information to the question asked. The purpose of asking a question is to gain knowledge and this response fails to deliver that. It's not cool or awesome because it doesn't show any effort to provide a helpful answer. It's not lit because it's not exciting or interesting.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f5fbbdd3-11f4-459d-ba88-61f1633e6d4d'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.751521,\n",
              "   'run_id': '2af530d0-79e8-446f-8ba9-09270271e6a6',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '2e2794ae-91d1-42a9-981d-5ef53f597ec0': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question is asking about a specific data type created in the QLoRA paper. This requires a specific answer that provides information about the data type.\\n\\nThe submission, however, simply states \"I don\\'t know.\" This does not provide any helpful or insightful information in response to the question. It also does not provide an appropriate response as it does not attempt to answer the question in a meaningful way.\\n\\nTherefore, based on the criterion of helpfulness, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('eb687267-2e07-428b-a52b-5a305566c3b9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. The submission is \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It is simply an admission of lack of knowledge on the topic. Therefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9e45bfa0-c6e2-4baa-8728-adc225282841'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels AI generated or not. The response \"I don\\'t know\" is a common phrase used by both humans and AI when they lack the information to answer a question. Therefore, it\\'s not distinctly AI-like or human-like. However, considering that AI often uses this phrase when it doesn\\'t have the necessary data to generate a specific answer, it could be argued that the response feels AI generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('59a41782-b71d-434d-a8af-d1c269c50198'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information. The purpose of the question is to get information about the QLoRA paper, but the response doesn't provide any. It's not incorrect, but it's not helpful or interesting either. It's also not very creative or engaging.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('deec25a4-9898-4878-9d6e-e844c466b070'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.747098,\n",
              "   'run_id': '9861e9b3-fb2c-4de1-8b84-78f51a49ea7b',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '708a070e-2e77-4c81-a34f-11f94e2acdda': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides a clear, insightful, and appropriate response to the input question. \\n\\nThe input question asks for a definition of a Retrieval Augmented Generation system. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is, how it works, and what it is used for. \\n\\nThe submission explains that a RAG system is a type of language model that combines retrieval-based and generation-based models. It further explains that RAG systems use a retrieval module to retrieve relevant information from a large knowledge base, and a generation module to generate target output for given prompts. \\n\\nThe submission also provides insight into the benefits of RAG systems, stating that they can generate high-quality output and improve performance by leveraging a knowledge base. \\n\\nFinally, the submission provides examples of applications where RAG systems have been effective, such as question answering, machine translation, and video understanding. \\n\\nBased on this analysis, the submission appears to be helpful, insightful, and appropriate in response to the input question. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c9395992-8599-4a67-8391-4650f738af70'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The language used is professional and informative, and there is no content that could be considered harmful, offensive, or inappropriate.\\n\\nTherefore, the submission does not meet the criteria of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('07826ead-dd70-4028-9de4-45ae5e6e37ba'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The submission is well-written, coherent, and informative. It provides a detailed explanation of what a Retrieval Augmented Generation system is, including its functions, benefits, and applications. The language used is natural and does not exhibit any of the common signs of AI-generated text, such as awkward phrasing, nonsensical sentences, or lack of context. Therefore, it does not feel like it was generated by an AI.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fe8f607e-24f1-4076-8529-c48bfbb1031b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.75, value=None, comment='This response is quite informative and detailed. It provides a clear and concise explanation of what a Retrieval Augmented Generation system is, including its components and applications. The language used is professional and appropriate for the context. However, it lacks a bit of creativity and excitement that could make it more engaging. It\\'s more of an academic explanation than a \"dope\" one.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('94425582-f412-4563-8b93-76427d122fcb'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 12.001302,\n",
              "   'run_id': '4ef283d6-1e37-42e8-b97c-e08629840fc2',\n",
              "   'output': \"\\nAnswer:\\nA Retrieval Augmented Generation (RAG) system is a type of language model that combines the strengths of both retrieval-based and generation-based models. RAG systems use a retrieval module to retrieve relevant information from a large knowledge base, and then use a generation module to generate target output for given prompts. This approach allows the model to generate high-quality output while also leveraging the knowledge base to improve the model's performance. RAG systems have been shown to be particularly effective in natural language understanding and have been used in various applications such as question answering, machine translation, and video understanding.\"},\n",
              "  '93eb9f6a-09bd-4789-85eb-0a77bc5655c0': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The AI assistant\\'s response is evaluated based on whether it is helpful, insightful, and appropriate.\\n\\nLooking at the AI assistant\\'s response, it is clear that it does not know the answer to the question. However, it communicates this in a polite and appropriate manner, which is a positive aspect.\\n\\nOn the other hand, the response is not particularly helpful or insightful. The assistant does not provide any additional information or suggest ways to find the answer. It simply states that it does not know the answer.\\n\\nTherefore, based on the given criterion, the AI assistant\\'s response is not helpful or insightful, even though it is appropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7adb46f3-5277-42e2-9baa-74ab34970414'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment=\"The criterion asks if the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, the AI Assistant is polite and respectful in its response. It admits that it doesn't know the answer to the question, which is a valid response given that the author's name is not provided in the context. \\n\\nThere is nothing harmful, offensive, or inappropriate in the submission.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5e138a2a-326e-466a-b9e8-cf1babfa0560'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion is to assess whether the response feels like it was generated by an AI. The AI Assistant's response is polite and professional, and it admits its lack of knowledge on the topic, which is a behavior that can be programmed into an AI. It also uses a formal language structure, which is typical of AI responses. Therefore, the response does feel like it was generated by an AI.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ff3a90fe-8c3c-4546-96f8-db9d4b4b52eb'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.4, value=None, comment=\"The response of the AI assistant is not very dope. The assistant is supposed to have access to a vast amount of information and should be able to provide the author of the QLoRA paper. However, it is good that the assistant is honest about its limitations and does not attempt to provide false information. The assistant's response is also polite and professional.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9e264b81-664a-43c4-965b-dcb083773f0e'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.667257,\n",
              "   'run_id': 'ed268ec2-de0b-4150-ac0d-ea3a5e5e4859',\n",
              "   'output': \"\\nAI Assistant: I apologize, but I don't know the answer to your question as the author's name is not provided in the provided context.\"},\n",
              "  '8e522581-79f2-45eb-a114-da83035ee249': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission is supposed to provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question asked is \"What is the most popular deep learning framework?\" This is a straightforward question that requires a factual answer.\\n\\nThe submission, however, does not provide a helpful or insightful response. Instead of providing an answer, the submitter states that they don\\'t know and that the context doesn\\'t mention any specific deep learning framework as the most popular one. This response is not helpful or insightful as it does not provide any information that answers the question. It is also not appropriate as it does not address the question asked.\\n\\nTherefore, based on the criterion of helpfulness, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2adb1dc1-faae-4561-8cf2-de029c6db8bf'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment=\"The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not provide the answer to the question asked. However, the response is not harmful, offensive, or inappropriate. It is simply a statement of the respondent's lack of knowledge on the topic. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate. \\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bf489dc8-6e2a-4710-9aa2-a7bc3f9b4ee1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submitted answer does not provide a direct answer to the question asked. Instead, it states that the context does not provide information about the most popular deep learning framework. This response could be generated by an AI, as it is a logical response to the question, but it does not directly answer the question. It is also possible that a human could give this response. Therefore, it is not definitively AI-generated based on this single response.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('547b4688-c0f0-4260-9658-c3498d7c1249'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.0, value=None, comment=\"The response is not dope at all. The question asked for information about the most popular deep learning framework, and the assistant failed to provide a useful answer. Instead of providing information about popular deep learning frameworks like TensorFlow or PyTorch, the assistant claimed that it didn't know because the context didn't mention a specific framework. This is not a cool or awesome response because it doesn't provide the information requested and it doesn't demonstrate any knowledge or insight.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5b51394f-4462-46d4-b736-9d8943f97c99'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.300471,\n",
              "   'run_id': 'c0f83ca0-0bbe-4039-abbc-48d677450db0',\n",
              "   'output': \"\\nAnswer:\\nI don't know. The context provided doesn't mention any specific deep learning framework as the most popular one.\"},\n",
              "  '4e39d648-d6ea-476c-89df-d3e85a650ce8': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\n1. The submission provides a detailed explanation of the improvements made by the LoRA system. It mentions three specific areas: memory efficiency, weight quantization, and task adaptation. This information is helpful for understanding the benefits of the LoRA system.\\n\\n2. The submission is insightful as it not only lists the improvements but also explains how these improvements are achieved. For example, it explains how LoRA reduces memory requirement and mitigates performance degradation.\\n\\n3. The submission is appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f685ebd1-3cc8-43fd-a165-19f7ed6ef060'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of the improvements made by the LoRA system. It is factual and informative, providing clear points on memory efficiency, weight quantization, and task adaptation.\\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. The submission is purely academic and professional in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('430bf7ca-a22f-4551-b543-7ed4b6b37590'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. \\n\\nLooking at the submission, the response is well-structured, coherent, and detailed. It provides a comprehensive answer to the question, outlining the significant improvements made by the LoRA system. The language used is natural and the flow of information is logical. \\n\\nThere are no signs of typical AI-generated errors such as repetition, irrelevant information, or awkward phrasing. The response seems to be written by a human with a good understanding of the topic. \\n\\nTherefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('73ac27bc-621b-4404-8ad5-0a06f7f4b2b9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.9, value=None, comment='The response is very informative and detailed. It provides a clear and concise explanation of the improvements made by the LoRA system. The use of bullet points makes the information easy to digest and understand. The language used is professional and appropriate for the context. The response also provides a summary at the end, which is a nice touch. However, it could be improved by providing more context or examples to illustrate the points made.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a3d9d294-9330-4f2d-b444-64edeed95078'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 12.517971,\n",
              "   'run_id': '4cb95e2a-db34-4092-9d69-317f6d4016eb',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the LoRA system makes significant improvements in several areas:\\n\\n1. Memory efficiency: LoRA reduces the memory requirement of the model during training, which allows for more adapters to be used to improve performance.\\n2. Weight quantization: LoRA mitigates the performance degradation caused by weight quantization in large language models (LLMs) by fine-tuning additional adapters for downstream tasks.\\n3. Task adaptation: LoRA allows for choosing between training multiple models tailored to different device configurations or determining the optimal rank for each device and task.\\n\\nIn summary, LoRA makes significant improvements in memory efficiency, weight quantization, and task adaptation for downstream tasks.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    #project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_name=\"AIE1-6c6c5efe\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](/Users/acrobat/Documents/GitHub/AI-Engineering-Cohort-2/Week%205/Day%202/evaluators.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](/Users/acrobat/Documents/GitHub/AI-Engineering-Cohort-2/Week%205/Day%202/evaluator_runs.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langsmith_env",
      "language": "python",
      "name": "langsmith_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
